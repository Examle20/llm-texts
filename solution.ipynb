{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3280bbde",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57378ce1",
   "metadata": {},
   "source": [
    "1) Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5213b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe28919",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/corpus\")\n",
    "OUT_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "\n",
    "MIN_SENT_CHARS = 20\n",
    "MAX_SENT_CHARS = 5000\n",
    "\n",
    "CONTEXT_LEN = 1024\n",
    "MAX_TOKENS_PER_CHUNK = CONTEXT_LEN - 2 \n",
    "\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"«\", '\"').replace(\"»\", '\"').replace(\"„\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"—\", \" — \")\n",
    "    text = text.replace(\"–\", \" — \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_punct(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
    "    s = re.sub(r\"!{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\?{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\?!){2,}\", \"?!\", s)\n",
    "    s = re.sub(r\",{2,}\", \",\", s)\n",
    "    s = re.sub(r\":{2,}\", \":\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([,.;:!?])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "CYR_RE = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def cyrillic_ratio(s):\n",
    "    letters = re.findall(r\"[A-Za-zА-Яа-яЁё]\", s)\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    cyr = sum(1 for ch in letters if CYR_RE.match(ch))\n",
    "    return cyr / len(letters)\n",
    "\n",
    "def is_good_sentence(s):\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if len(s) < MIN_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if len(s) > MAX_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if LATIN_RE.search(s):\n",
    "        return False\n",
    "    \n",
    "    if cyrillic_ratio(s) < 0.70:\n",
    "        return False\n",
    "    \n",
    "    letters_count = len(CYR_RE.findall(s))\n",
    "    if letters_count < 5:\n",
    "        return False\n",
    "    \n",
    "    if len(set(s)) <= 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_for_dedup(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9а-яё]+\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        return len(text.split())\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_sentences(sentences, max_tokens, tokenizer=None):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = count_tokens(s, tokenizer)\n",
    "        if s_tokens > max_tokens:\n",
    "            words = s.split()\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for w in words:\n",
    "                w_tokens = count_tokens(w, tokenizer)\n",
    "                if buf_tokens + w_tokens > max_tokens and buf:\n",
    "                    chunks.append(\" \".join(buf))\n",
    "                    buf = [w]\n",
    "                    buf_tokens = w_tokens\n",
    "                else:\n",
    "                    buf.append(w)\n",
    "                    buf_tokens += w_tokens\n",
    "\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + s_tokens > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            current_tokens = s_tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            current_tokens += s_tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecf0633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество файлов: 108\n",
      "=== STATS ===\n",
      "docs_total: 108\n",
      "docs_unique: 107\n",
      "sents_total: 537738\n",
      "sents_good: 441048\n",
      "sents_unique: 436042\n",
      "chunks_total: 6386\n",
      "Saved: data\\pretrain_corpus.jsonl chunks: 6386\n"
     ]
    }
   ],
   "source": [
    "txt_files = sorted(DATA_PATH.glob(\"*.txt\"))\n",
    "print(\"Количество файлов:\", len(txt_files))\n",
    "\n",
    "seen_docs = set()\n",
    "seen_sents = set()\n",
    "all_chunks = []\n",
    "\n",
    "stats = {\n",
    "    \"docs_total\": 0,\n",
    "    \"docs_unique\": 0,\n",
    "    \"sents_total\": 0,\n",
    "    \"sents_good\": 0,\n",
    "    \"sents_unique\": 0,\n",
    "    \"chunks_total\": 0\n",
    "}\n",
    "\n",
    "for fp in txt_files:\n",
    "    stats[\"docs_total\"] += 1\n",
    "\n",
    "    raw = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = normalize_text(raw)\n",
    "\n",
    "    doc_key = sha1(normalize_for_dedup(raw))\n",
    "    if doc_key in seen_docs:\n",
    "        continue\n",
    "\n",
    "    seen_docs.add(doc_key)\n",
    "    stats[\"docs_unique\"] += 1\n",
    "\n",
    "    sents = split_sentences(raw)\n",
    "    stats[\"sents_total\"] += len(sents)\n",
    "\n",
    "    cleaned = []\n",
    "    for s in sents:\n",
    "        s = normalize_punct(s)\n",
    "        if not is_good_sentence(s):\n",
    "            continue\n",
    "\n",
    "        stats[\"sents_good\"] += 1\n",
    "\n",
    "        sent_key = sha1(normalize_for_dedup(s))\n",
    "        if sent_key in seen_sents:\n",
    "            continue\n",
    "\n",
    "        seen_sents.add(sent_key)\n",
    "        stats[\"sents_unique\"] += 1\n",
    "        cleaned.append(s)\n",
    "\n",
    "    chunks = chunk_sentences(cleaned, max_tokens=MAX_TOKENS_PER_CHUNK, tokenizer=None)\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = f\"{BOS} {ch.strip()} {EOS}\"\n",
    "        all_chunks.append(text)\n",
    "\n",
    "stats[\"chunks_total\"] = len(all_chunks)\n",
    "\n",
    "print(\"=== STATS ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for t in all_chunks:\n",
    "        f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcae1a2",
   "metadata": {},
   "source": [
    "3) Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843125e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово. Строк для обучения токенизатора: 6386\n",
      "Файл: data\\tokenizer_train.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "JSONL_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "TXT_TRAIN_PATH = Path(\"./data/tokenizer_train.txt\")\n",
    "\n",
    "TXT_TRAIN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f_in, open(TXT_TRAIN_PATH, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        f_out.write(text.replace(\"\\n\", \" \") + \"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Готово. Строк для обучения токенизатора:\", count)\n",
    "print(\"Файл:\", TXT_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9eff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer.json: tokenizer_bpe_3k\\tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "VOCAB_SIZE = 3000\n",
    "MIN_FREQUENCY = 2\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[str(TXT_TRAIN_PATH)],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "\n",
    "OUT_DIR = Path(\"./tokenizer_bpe_3k\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer.save_model(str(OUT_DIR))\n",
    "\n",
    "tokenizer.save(str(OUT_DIR / \"tokenizer.json\"))\n",
    "print(\"Saved tokenizer.json:\", OUT_DIR / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e28689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 6386 examples [00:00, 37929.39 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:07<00:00, 864.89 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:13<00:00, 458.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 26878\n",
      "})\n",
      "Примеров: 26878\n",
      "Длина блока: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\": \"./data/pretrain_corpus.jsonl\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "tokenized = ds[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_input_ids = []\n",
    "    concatenated_attention = []\n",
    "\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated_input_ids.extend(ids)\n",
    "\n",
    "    for am in examples[\"attention_mask\"]:\n",
    "        concatenated_attention.extend(am)\n",
    "\n",
    "    total_length = len(concatenated_input_ids)\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, total_length, BLOCK_SIZE):\n",
    "        chunk_ids = concatenated_input_ids[i : i + BLOCK_SIZE]\n",
    "        chunk_mask = concatenated_attention[i : i + BLOCK_SIZE]\n",
    "\n",
    "        input_ids.append(chunk_ids)\n",
    "        attention_mask.append(chunk_mask)\n",
    "        labels.append(chunk_ids.copy())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "lm_dataset = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized.column_names\n",
    ")\n",
    "\n",
    "print(lm_dataset)\n",
    "print(\"Примеров:\", len(lm_dataset))\n",
    "print(\"Длина блока:\", len(lm_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501278",
   "metadata": {},
   "source": [
    "4. Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3000\n",
      "Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "Total params: 132006912\n",
      "Trainable params: 132006912\n",
      "~M params: 132.01 M\n",
      "Logits shape: torch.Size([2, 32, 3000])\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    rms_norm_eps=1e-6,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total params:\", total_params)\n",
    "print(\"Trainable params:\", trainable_params)\n",
    "print(\"~M params:\", round(total_params / 1e6, 2), \"M\")\n",
    "\n",
    "# 5) Санити-чек forward pass\n",
    "model.eval()\n",
    "x = torch.randint(0, tokenizer.vocab_size, (2, 32))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=x)\n",
    "print(\"Logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce82bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 26340\n",
      "Val size: 538\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]\n",
    "\n",
    "splits = lm_dataset.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PromptGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, prompts, tokenizer, max_new_tokens=80):\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        device = model.device\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[Eval @ step {state.global_step}] Prompt generations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        old_cache = getattr(model.config, \"use_cache\", False)\n",
    "        model.config.use_cache = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                out_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"\\n--- Prompt #{i+1} ---\")\n",
    "                print(\"PROMPT:\", prompt)\n",
    "                print(\"GEN:\\n\", text)\n",
    "\n",
    "        model.config.use_cache = old_cache\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ec39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zinov\\AppData\\Local\\Temp\\ipykernel_9372\\623737745.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model.config.use_cache = False\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "callbacks = [PromptGenerationCallback(test_prompts, tokenizer, max_new_tokens=80)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b47d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='412' max='412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [412/412 16:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Eval @ step 412] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия на лампе и, с тем же время с другим, как будто у нее в то что не мог в нем не замечать, что в доме его всегда были преступники. Ирина тоже были уже и теперь, хотя бы у меня было бы это, и он никогда не знаю, а потом с ним отечествия, и с ней, в вашем мир\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа в том, чтобы не наступаться. Лебедев, что он самым образом был безнадежствовать его, то, потому что он был уже не имеет. \n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания на площадке. -- Глаза, -- сказал Нехлюдов, нехорошая, -- подойдя к вам. Другой рукой вказывающих походка с коктка, которые лугались на него отрезала в гостиную, за сила с тюрьма и режет в сущности подчиненного к\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным в городе. Я его не любил, в том, что он нарочно и приставился с Серебряном панели; и если бы я не знала, как всегда в чем дело и никак не могла сделать, когда все эти недельности я уже не был так скоро и как будто в то, чтобы он не понимал. Да для всех, у него\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду с ним. А в ней и с кипиейкой к нему отзывался от него, что она спала на фригада, он не решился ее, как и всякому человеку отдачно. Очень владеть им по ночам его буловой полки. Все это было разнимать, но это было быть. Он\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти, что она его не может, относить его. Мужиков была очень видны, и в нем не только о чем он, но не мог ни слова ни себя, ни на чем он ни на то, что это время. А главнее было, как и они были его чувства, чем он обманул к вам, и на нее не только с другими словами\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена в лавке; но не было: в этом минуту я был не на себе, что ты она к нему бала на нее? .. Сони я уехал, я только не сказал, чтобы я за ней не видел. Так он хотел и теперь очень хорошо, и я вас не умею. О, вот! \" -- сказала она; она теперь с вами пора; я\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая сила на нее, и в тела ее чувства, она была естественность к нему; я только не понимала ему и не то, чтобы не так хороша. Я думал только несколько шагов и проворно. Он опять открыла, что он его привязывался на место, что в нее был ее защитил его в ее слове\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело, что, удачно будет иметь. Свящайся дыбогодый, - поди насчет он, а на цинке, он уже спорил в комнату: \"Мой на него\". - Душевое, я вам употребил его. Он как-то отглядел, но не спрятал, а\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно, что не только подавали с ним, и то, как ты от него. Главная, кажется, да с того до сих пор, если б я, - только тут я. А впрочем, когда он мне было в том, чтобы ты должен было бы ни один в чем не верю, чтобы все еще, а я не знаю, что они бы не мог так.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "{'eval_loss': 4.099081993103027, 'eval_runtime': 8.0435, 'eval_samples_per_second': 66.887, 'eval_steps_per_second': 8.454, 'epoch': 1.0}\n",
      "Perplexity: 60.28492021489011\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "if \"eval_loss\" in metrics:\n",
    "    print(\"Perplexity:\", math.exp(metrics[\"eval_loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
