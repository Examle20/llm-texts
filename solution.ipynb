{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3280bbde",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57378ce1",
   "metadata": {},
   "source": [
    "1) Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe28919",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/corpus\")\n",
    "OUT_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "\n",
    "MIN_SENT_CHARS = 20\n",
    "MAX_SENT_CHARS = 5000\n",
    "\n",
    "CONTEXT_LEN = 1024\n",
    "MAX_TOKENS_PER_CHUNK = CONTEXT_LEN - 2 \n",
    "\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"«\", '\"').replace(\"»\", '\"').replace(\"„\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"—\", \" — \")\n",
    "    text = text.replace(\"–\", \" — \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_punct(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
    "    s = re.sub(r\"!{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\?{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\?!){2,}\", \"?!\", s)\n",
    "    s = re.sub(r\",{2,}\", \",\", s)\n",
    "    s = re.sub(r\":{2,}\", \":\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([,.;:!?])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "CYR_RE = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def cyrillic_ratio(s):\n",
    "    letters = re.findall(r\"[A-Za-zА-Яа-яЁё]\", s)\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    cyr = sum(1 for ch in letters if CYR_RE.match(ch))\n",
    "    return cyr / len(letters)\n",
    "\n",
    "def is_good_sentence(s):\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if len(s) < MIN_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if len(s) > MAX_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if LATIN_RE.search(s):\n",
    "        return False\n",
    "    \n",
    "    if cyrillic_ratio(s) < 0.70:\n",
    "        return False\n",
    "    \n",
    "    letters_count = len(CYR_RE.findall(s))\n",
    "    if letters_count < 5:\n",
    "        return False\n",
    "    \n",
    "    if len(set(s)) <= 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_for_dedup(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9а-яё]+\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        return len(text.split())\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_sentences(sentences, max_tokens, tokenizer=None):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = count_tokens(s, tokenizer)\n",
    "        if s_tokens > max_tokens:\n",
    "            words = s.split()\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for w in words:\n",
    "                w_tokens = count_tokens(w, tokenizer)\n",
    "                if buf_tokens + w_tokens > max_tokens and buf:\n",
    "                    chunks.append(\" \".join(buf))\n",
    "                    buf = [w]\n",
    "                    buf_tokens = w_tokens\n",
    "                else:\n",
    "                    buf.append(w)\n",
    "                    buf_tokens += w_tokens\n",
    "\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + s_tokens > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            current_tokens = s_tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            current_tokens += s_tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = sorted(DATA_PATH.glob(\"*.txt\"))\n",
    "print(\"Количество файлов:\", len(txt_files))\n",
    "\n",
    "seen_docs = set()\n",
    "seen_sents = set()\n",
    "all_chunks = []\n",
    "\n",
    "stats = {\n",
    "    \"docs_total\": 0,\n",
    "    \"docs_unique\": 0,\n",
    "    \"sents_total\": 0,\n",
    "    \"sents_good\": 0,\n",
    "    \"sents_unique\": 0,\n",
    "    \"chunks_total\": 0\n",
    "}\n",
    "\n",
    "for fp in txt_files:\n",
    "    stats[\"docs_total\"] += 1\n",
    "\n",
    "    raw = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = normalize_text(raw)\n",
    "\n",
    "    doc_key = sha1(normalize_for_dedup(raw))\n",
    "    if doc_key in seen_docs:\n",
    "        continue\n",
    "\n",
    "    seen_docs.add(doc_key)\n",
    "    stats[\"docs_unique\"] += 1\n",
    "\n",
    "    sents = split_sentences(raw)\n",
    "    stats[\"sents_total\"] += len(sents)\n",
    "\n",
    "    cleaned = []\n",
    "    for s in sents:\n",
    "        s = normalize_punct(s)\n",
    "        if not is_good_sentence(s):\n",
    "            continue\n",
    "\n",
    "        stats[\"sents_good\"] += 1\n",
    "\n",
    "        sent_key = sha1(normalize_for_dedup(s))\n",
    "        if sent_key in seen_sents:\n",
    "            continue\n",
    "\n",
    "        seen_sents.add(sent_key)\n",
    "        stats[\"sents_unique\"] += 1\n",
    "        cleaned.append(s)\n",
    "\n",
    "    chunks = chunk_sentences(cleaned, max_tokens=MAX_TOKENS_PER_CHUNK, tokenizer=None)\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = f\"{BOS} {ch.strip()} {EOS}\"\n",
    "        all_chunks.append(text)\n",
    "\n",
    "stats[\"chunks_total\"] = len(all_chunks)\n",
    "\n",
    "print(\"=== STATS ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for t in all_chunks:\n",
    "        f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcae1a2",
   "metadata": {},
   "source": [
    "3) Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843125e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "JSONL_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "TXT_TRAIN_PATH = Path(\"./data/tokenizer_train.txt\")\n",
    "\n",
    "TXT_TRAIN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f_in, open(TXT_TRAIN_PATH, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        f_out.write(text.replace(\"\\n\", \" \") + \"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Готово. Строк для обучения токенизатора:\", count)\n",
    "print(\"Файл:\", TXT_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "VOCAB_SIZE = 3000\n",
    "MIN_FREQUENCY = 2\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[str(TXT_TRAIN_PATH)],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "\n",
    "OUT_DIR = Path(\"./tokenizer_bpe_3k\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer.save_model(str(OUT_DIR))\n",
    "\n",
    "tokenizer.save(str(OUT_DIR / \"tokenizer.json\"))\n",
    "print(\"Saved tokenizer.json:\", OUT_DIR / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e28689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\": \"./data/pretrain_corpus.jsonl\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "tokenized = ds[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_input_ids = []\n",
    "    concatenated_attention = []\n",
    "\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated_input_ids.extend(ids)\n",
    "\n",
    "    for am in examples[\"attention_mask\"]:\n",
    "        concatenated_attention.extend(am)\n",
    "\n",
    "    total_length = len(concatenated_input_ids)\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, total_length, BLOCK_SIZE):\n",
    "        chunk_ids = concatenated_input_ids[i : i + BLOCK_SIZE]\n",
    "        chunk_mask = concatenated_attention[i : i + BLOCK_SIZE]\n",
    "\n",
    "        input_ids.append(chunk_ids)\n",
    "        attention_mask.append(chunk_mask)\n",
    "        labels.append(chunk_ids.copy())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "lm_dataset = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized.column_names\n",
    ")\n",
    "\n",
    "print(lm_dataset)\n",
    "print(\"Примеров:\", len(lm_dataset))\n",
    "print(\"Длина блока:\", len(lm_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501278",
   "metadata": {},
   "source": [
    "4. Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    rms_norm_eps=1e-6,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total params:\", total_params)\n",
    "print(\"Trainable params:\", trainable_params)\n",
    "print(\"~M params:\", round(total_params / 1e6, 2), \"M\")\n",
    "\n",
    "model.eval()\n",
    "x = torch.randint(0, tokenizer.vocab_size, (2, 32))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=x)\n",
    "print(\"Logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce82bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]\n",
    "\n",
    "splits = lm_dataset.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PromptGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, prompts, tokenizer, max_new_tokens=80):\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        device = model.device\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[Eval @ step {state.global_step}] Prompt generations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        old_cache = getattr(model.config, \"use_cache\", False)\n",
    "        model.config.use_cache = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                out_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"\\n--- Prompt #{i+1} ---\")\n",
    "                print(\"PROMPT:\", prompt)\n",
    "                print(\"GEN:\\n\", text)\n",
    "\n",
    "        model.config.use_cache = old_cache\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ec39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model.config.use_cache = False\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "callbacks = [PromptGenerationCallback(test_prompts, tokenizer, max_new_tokens=80)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "if \"eval_loss\" in metrics:\n",
    "    print(\"Perplexity:\", math.exp(metrics[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b359a9",
   "metadata": {},
   "source": [
    "# Post-train SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def generate_answer(question, max_new_tokens=120):\n",
    "    prompt = f\"Вопрос: {question}\\nОтвет:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Ответ:\" in text:\n",
    "        return text.split(\"Ответ:\", 1)[-1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "print(\"=== Base Model (Before SFT) Output ===\\n\")\n",
    "\n",
    "for i, q in enumerate(questions_rus, 1):\n",
    "    ans = generate_answer(q)\n",
    "\n",
    "    print(f\"Model Input {i}:\")\n",
    "    print(q)\n",
    "    print(f\"Model Output {i}:\")\n",
    "    print(ans)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ac20e",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70f1c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'instruction', 'output'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n",
      "Train size: 50724\n",
      "Val size: 1036\n",
      "Example messages:\n",
      " [{'content': 'Ты полезный ассистент. Отвечай по-русски, кратко и по делу.', 'role': 'system'}, {'content': 'Какую последнюю операцию вы ожидаете выполнить при обучении модели машинного обучения?', 'role': 'user'}, {'content': 'Последняя операция, которую можно ожидать при обучении модели машинного обучения, — это оценка производительности модели на проверочном или тестовом наборе данных. Этот шаг включает в себя использование обученной модели для прогнозирования набора данных, которых она раньше не видела, а затем сравнение этих прогнозов с фактическими результатами для оценки точности и способности модели к обобщению. Это позволяет точно настроить гиперпараметры модели перед ее развертыванием на практике или использованием для прогнозирования новых данных.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай по-русски, кратко и по делу.\"\n",
    "\n",
    "ds = load_dataset(\"d0rj/alpaca-cleaned-ru\")\n",
    "print(ds)\n",
    "\n",
    "def build_user_text(instruction, inp):\n",
    "    instruction = (instruction or \"\").strip()\n",
    "    inp = (inp or \"\").strip()\n",
    "    if inp:\n",
    "        return instruction + \"\\n\\nКонтекст:\\n\" + inp\n",
    "    return instruction\n",
    "\n",
    "def to_messages(example):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "\n",
    "    user_text = build_user_text(instruction, inp)\n",
    "    assistant_text = (output or \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "train_chat = ds[\"train\"].map(to_messages, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "split = train_chat.train_test_split(test_size=0.02, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "print(\"Example messages:\\n\", train_ds[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8044da",
   "metadata": {},
   "source": [
    "# Дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfc7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: Qwen/Qwen2.5-0.5B\n",
      "Model dtype: torch.float32\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float32,   # <-- ВАЖНО: FP32 чтобы не было \"Attempting to unscale FP16 gradients\"\n",
    ")\n",
    "\n",
    "# обязательно для обучения\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Tokenizer:\", tokenizer.name_or_path)\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype)\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Baseline check BEFORE SFT (optional, but полезно)\n",
    "# =========================\n",
    "def generate_answers(title):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(f\"=== {title} ===\\n\")\n",
    "\n",
    "    for i, q in enumerate(questions_rus, 1):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "        ]\n",
    "\n",
    "        # для Qwen используем chat template\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=120,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        print(f\"Model Input {i}:\")\n",
    "        print(q)\n",
    "        print(f\"Model Output {i}:\")\n",
    "        print(text)\n",
    "        print()\n",
    "\n",
    "\n",
    "class QuestionsCallback(TrainerCallback):\n",
    "    def __init__(self, questions, tokenizer, title, max_new_tokens=120):\n",
    "        self.questions = questions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.title = title\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        print(f\"\\n=== {self.title} ===\\n\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, q in enumerate(self.questions, 1):\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": q},\n",
    "                ]\n",
    "\n",
    "                prompt = self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "                text = self.tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "                print(f\"Model Input {i}:\")\n",
    "                print(q)\n",
    "                print(f\"Model Output {i}:\")\n",
    "                print(text)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e90ae55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT fp16: True\n",
      "SFT bf16: False\n",
      "Effective batch: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='793' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/793 03:04 < 40:26:57, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     52\u001b[39m trainer = SFTTrainer(\n\u001b[32m     53\u001b[39m     model=model,\n\u001b[32m     54\u001b[39m     args=sft_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     callbacks=[QuestionsCallback(questions_rus, tokenizer, title=\u001b[33m\"\u001b[39m\u001b[33mBase Model (After SFT) Output\u001b[39m\u001b[33m\"\u001b[39m, max_new_tokens=\u001b[32m120\u001b[39m)]\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# 9) Train\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:1264\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm-texts\\llm-texts\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:1183\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1181\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1182\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in inputs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m         entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather_for_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m._metrics[mode][\u001b[33m\"\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m\"\u001b[39m].append(entropy)\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1187\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m   1188\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "import inspect\n",
    "import torch\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 32  # 2*32 = 64 effective\n",
    "\n",
    "sig = inspect.signature(SFTConfig)\n",
    "use_eval_strategy = \"eval_strategy\" in sig.parameters\n",
    "\n",
    "cfg_kwargs = dict(\n",
    "    output_dir=\"./qwen_sft_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    report_to=\"none\",\n",
    "\n",
    "    bf16=False,  # <-- для 2080 Ti\n",
    "    fp16=True,   # <-- используем AMP fp16 через Trainer\n",
    ")\n",
    "\n",
    "if use_eval_strategy:\n",
    "    cfg_kwargs[\"eval_strategy\"] = \"steps\"\n",
    "else:\n",
    "    cfg_kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "\n",
    "sft_config = SFTConfig(**cfg_kwargs)\n",
    "\n",
    "print(\"SFT fp16:\", sft_config.fp16)\n",
    "print(\"SFT bf16:\", sft_config.bf16)\n",
    "print(\"Effective batch:\", per_device_train_batch_size * gradient_accumulation_steps)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Trainer\n",
    "# =========================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[QuestionsCallback(questions_rus, tokenizer, title=\"Base Model (After SFT) Output\", max_new_tokens=120)]\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 9) Train\n",
    "# =========================\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
