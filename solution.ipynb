{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3280bbde",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57378ce1",
   "metadata": {},
   "source": [
    "1) Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5213b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe28919",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/corpus\")\n",
    "OUT_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "\n",
    "MIN_SENT_CHARS = 20\n",
    "MAX_SENT_CHARS = 5000\n",
    "\n",
    "CONTEXT_LEN = 1024\n",
    "MAX_TOKENS_PER_CHUNK = CONTEXT_LEN - 2 \n",
    "\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"«\", '\"').replace(\"»\", '\"').replace(\"„\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"—\", \" — \")\n",
    "    text = text.replace(\"–\", \" — \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_punct(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
    "    s = re.sub(r\"!{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\?{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\?!){2,}\", \"?!\", s)\n",
    "    s = re.sub(r\",{2,}\", \",\", s)\n",
    "    s = re.sub(r\":{2,}\", \":\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([,.;:!?])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "CYR_RE = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def cyrillic_ratio(s):\n",
    "    letters = re.findall(r\"[A-Za-zА-Яа-яЁё]\", s)\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    cyr = sum(1 for ch in letters if CYR_RE.match(ch))\n",
    "    return cyr / len(letters)\n",
    "\n",
    "def is_good_sentence(s):\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if len(s) < MIN_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if len(s) > MAX_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if LATIN_RE.search(s):\n",
    "        return False\n",
    "    \n",
    "    if cyrillic_ratio(s) < 0.70:\n",
    "        return False\n",
    "    \n",
    "    letters_count = len(CYR_RE.findall(s))\n",
    "    if letters_count < 5:\n",
    "        return False\n",
    "    \n",
    "    if len(set(s)) <= 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_for_dedup(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9а-яё]+\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        return len(text.split())\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_sentences(sentences, max_tokens, tokenizer=None):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = count_tokens(s, tokenizer)\n",
    "        if s_tokens > max_tokens:\n",
    "            words = s.split()\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for w in words:\n",
    "                w_tokens = count_tokens(w, tokenizer)\n",
    "                if buf_tokens + w_tokens > max_tokens and buf:\n",
    "                    chunks.append(\" \".join(buf))\n",
    "                    buf = [w]\n",
    "                    buf_tokens = w_tokens\n",
    "                else:\n",
    "                    buf.append(w)\n",
    "                    buf_tokens += w_tokens\n",
    "\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + s_tokens > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            current_tokens = s_tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            current_tokens += s_tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecf0633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество файлов: 108\n",
      "=== STATS ===\n",
      "docs_total: 108\n",
      "docs_unique: 107\n",
      "sents_total: 537738\n",
      "sents_good: 441048\n",
      "sents_unique: 436042\n",
      "chunks_total: 6386\n",
      "Saved: data/pretrain_corpus.jsonl chunks: 6386\n"
     ]
    }
   ],
   "source": [
    "txt_files = sorted(DATA_PATH.glob(\"*.txt\"))\n",
    "print(\"Количество файлов:\", len(txt_files))\n",
    "\n",
    "seen_docs = set()\n",
    "seen_sents = set()\n",
    "all_chunks = []\n",
    "\n",
    "stats = {\n",
    "    \"docs_total\": 0,\n",
    "    \"docs_unique\": 0,\n",
    "    \"sents_total\": 0,\n",
    "    \"sents_good\": 0,\n",
    "    \"sents_unique\": 0,\n",
    "    \"chunks_total\": 0\n",
    "}\n",
    "\n",
    "for fp in txt_files:\n",
    "    stats[\"docs_total\"] += 1\n",
    "\n",
    "    raw = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = normalize_text(raw)\n",
    "\n",
    "    doc_key = sha1(normalize_for_dedup(raw))\n",
    "    if doc_key in seen_docs:\n",
    "        continue\n",
    "\n",
    "    seen_docs.add(doc_key)\n",
    "    stats[\"docs_unique\"] += 1\n",
    "\n",
    "    sents = split_sentences(raw)\n",
    "    stats[\"sents_total\"] += len(sents)\n",
    "\n",
    "    cleaned = []\n",
    "    for s in sents:\n",
    "        s = normalize_punct(s)\n",
    "        if not is_good_sentence(s):\n",
    "            continue\n",
    "\n",
    "        stats[\"sents_good\"] += 1\n",
    "\n",
    "        sent_key = sha1(normalize_for_dedup(s))\n",
    "        if sent_key in seen_sents:\n",
    "            continue\n",
    "\n",
    "        seen_sents.add(sent_key)\n",
    "        stats[\"sents_unique\"] += 1\n",
    "        cleaned.append(s)\n",
    "\n",
    "    chunks = chunk_sentences(cleaned, max_tokens=MAX_TOKENS_PER_CHUNK, tokenizer=None)\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = f\"{BOS} {ch.strip()} {EOS}\"\n",
    "        all_chunks.append(text)\n",
    "\n",
    "stats[\"chunks_total\"] = len(all_chunks)\n",
    "\n",
    "print(\"=== STATS ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for t in all_chunks:\n",
    "        f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcae1a2",
   "metadata": {},
   "source": [
    "3) Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843125e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово. Строк для обучения токенизатора: 6386\n",
      "Файл: data/tokenizer_train.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "JSONL_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "TXT_TRAIN_PATH = Path(\"./data/tokenizer_train.txt\")\n",
    "\n",
    "TXT_TRAIN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f_in, open(TXT_TRAIN_PATH, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        f_out.write(text.replace(\"\\n\", \" \") + \"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Готово. Строк для обучения токенизатора:\", count)\n",
    "print(\"Файл:\", TXT_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9eff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Saved tokenizer.json: tokenizer_bpe_3k/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "VOCAB_SIZE = 3000\n",
    "MIN_FREQUENCY = 2\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[str(TXT_TRAIN_PATH)],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "\n",
    "OUT_DIR = Path(\"./tokenizer_bpe_3k\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer.save_model(str(OUT_DIR))\n",
    "\n",
    "tokenizer.save(str(OUT_DIR / \"tokenizer.json\"))\n",
    "print(\"Saved tokenizer.json:\", OUT_DIR / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e28689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 6386 examples [00:00, 40259.95 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:05<00:00, 1086.98 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:11<00:00, 569.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 26876\n",
      "})\n",
      "Примеров: 26876\n",
      "Длина блока: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\": \"./data/pretrain_corpus.jsonl\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "tokenized = ds[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_input_ids = []\n",
    "    concatenated_attention = []\n",
    "\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated_input_ids.extend(ids)\n",
    "\n",
    "    for am in examples[\"attention_mask\"]:\n",
    "        concatenated_attention.extend(am)\n",
    "\n",
    "    total_length = len(concatenated_input_ids)\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, total_length, BLOCK_SIZE):\n",
    "        chunk_ids = concatenated_input_ids[i : i + BLOCK_SIZE]\n",
    "        chunk_mask = concatenated_attention[i : i + BLOCK_SIZE]\n",
    "\n",
    "        input_ids.append(chunk_ids)\n",
    "        attention_mask.append(chunk_mask)\n",
    "        labels.append(chunk_ids.copy())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "lm_dataset = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized.column_names\n",
    ")\n",
    "\n",
    "print(lm_dataset)\n",
    "print(\"Примеров:\", len(lm_dataset))\n",
    "print(\"Длина блока:\", len(lm_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501278",
   "metadata": {},
   "source": [
    "4. Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d04509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3000\n",
      "Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "Total params: 132006912\n",
      "Trainable params: 132006912\n",
      "Params: 132.01\n",
      "Logits shape: torch.Size([2, 32, 3000])\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    rms_norm_eps=1e-6,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total params:\", total_params)\n",
    "print(\"Trainable params:\", trainable_params)\n",
    "print(\"Params:\", round(total_params / 1e6, 2))\n",
    "\n",
    "model.eval()\n",
    "x = torch.randint(0, tokenizer.vocab_size, (2, 32))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=x)\n",
    "print(\"Logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce82bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8c1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 26338\n",
      "Val size: 538\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]\n",
    "\n",
    "splits = lm_dataset.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c057ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PromptGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, prompts, tokenizer, max_new_tokens=80):\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        device = model.device\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[Eval @ step {state.global_step}] Prompt generations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        old_cache = getattr(model.config, \"use_cache\", False)\n",
    "        model.config.use_cache = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                out_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"\\n--- Prompt #{i+1} ---\")\n",
    "                print(\"PROMPT:\", prompt)\n",
    "                print(\"GEN:\\n\", text)\n",
    "\n",
    "        model.config.use_cache = old_cache\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3ec39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19241/879651457.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model.config.use_cache = False\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "callbacks = [PromptGenerationCallback(test_prompts, tokenizer, max_new_tokens=80)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b47d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1236' max='1236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1236/1236 10:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.579600</td>\n",
       "      <td>3.608132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.118000</td>\n",
       "      <td>3.316448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Eval @ step 500] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия на лечение ее существ, - для меня все остальное, чтобы уходить директором. Их всегда, что эта история наружность и что не достигаемая фраза жизни\". Он читал, но когда он остроумно обижался, она еще села в супруг, и, наконец, в сущности\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа в Вене, на площадку с черным крохотным ножом. И все это были так близко: он, как всегда, спрятал у себя шпарки. Кучер, разнообразные лекции (\"только не могу забывать\", - со вздохами подумал Логин с подчеркнутой роскош\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания в школе, что это такое же слово, -- то есть к отцу, в первых дней и в сущности соображала его; но в сущности он только хотел знать ее в жизни, как будто желая уезжать в своем деле, даже на все недоверчиво, неуместная, но совершенно безопасная для него. Врон\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным, с трудом ожидает его: -- Узнать, что есть такие мармированы. Танкред видел, как только он, когда в комнате, ушел, у него как будто бы тягучее и ясное лицо пела к дому на груди, глаза ее, все еще ближе становились на клетчатые мукушки, и\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду перешагнуться с места в отделении. Я не понимаю ее, он стал уверен, что он уже не видит. .. Он умел думать, что его жена меня любит. Он очень любил его. Она сказала это: — Вы все-таки обманываете эту новость! Пожалуй, я тебя не слыхал. Но она сама решила\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти. На немецкое, наивное, почти злобно, точно, как и они, и не могут жить. Это так просто и весело обожало его: это было даже очень жажда. И потом как будто не было ничего, что он так любит меня на себе, что не только к тому, что все к тому, когда у него там уединилась. Это\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена, и потому она по ее мнениям не мог. Этот человек очень понравится, когда она будет со мной и как в этом году. Я только теперь имел то же, чего это было бы и на меня смотреть в Петербург, а еще более как будто он говорит: \"Хоть вы сами уже заходите\". Ну так и то#есть да что вы,\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая идеальная комната. В эту минуту он чувствовал это в глазах ее речи, когда его был очень больный вопрос: \"бежать! .. .\" И вдруг его не хотелось бы говорить: \"А у меня она не пишет, что я с ним в виде его! \" -- \"Мне так много\", но она меня не хотела бы было, чтобы она\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело. Воспользование председательнейшими людьми, я всегда в жизни, когда у меня есть в мире отличие и осуждения. Строим мы с ними не поет ни. Я так не видел, что в этой средине. Мне было сказано: я был сокровен. При чем больше я, как только\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно, я должен был быть женой; но когда я в нем думал с вами, когда я предлагаю правду, чтобы так и выйти из Москвы к моим братам. Но все-таки и я с ужасом уверен, что у меня есть до того, как пройдет через голову, на базаре. И что вы не поворачиваете\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[Eval @ step 1000] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия. До сего дня мы оба вышли в кабинет. За это время уже можно было видеть Лизавету Прокофьевну. — А я вас не знаю, ты мне не нравился, чтобы мне не хочется меня видеть, -- прошептала она, взглянув на Марью Николаевну, которая хотела поднять глаза и с выражением недовольного смущения\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа и, как всегда, невыносимо, только по одному чувству. Да и то, что, при котором не было ни одного слова, он не мог понимать этого. .. Но Левин видел, что это была та самая особенная мысль, которую он знал. Кити уже были в новом положении. Но теперь это она поняла. Она думала о любви к\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания в роли у ворот. \"Куда я тут? \" спросил он у подъезжавшего канавки. Узнав его, он сказал: - Вчера вечером, если б вы знали в нем новое время, я бы вам скажу, чтобы я тебе сейчас же узнал. Он взглянул на меня в сизый час со всеми кас\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным, что я ничего не могу жить в другом, не знаю. И наше время: у кого же мне супруга? Зачем же ты теперь знаешь, что для меня, по крайней мере, неужели? Ведь я все-таки хочу тебя прожить. Я уже не буду больше и не буду, когда я влюбилась, хотя и думала, что\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду ходить. В это время кто-то засмеялся. Приподнялась пешком и тихо сказала: -- Ей что же дальше, -- вот какие! И снова собралась женщина, бросила письмо на постель, потом по лестнице, к трюмо, и быстро пошел прочь от дыхания. Он стоял передо мною, и, наконец,\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти и не давать ему возможным. .. -- сказал отец Андрей, подтвердительно глядя на него. Он встал и с досадой поглядел на старика в черной квадрате, стоявшей у ворот; и, по-моему, старухе он еще не успел заметить, как казалось, что все они делают ему, но тот, весь\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена: что будет в бока? .. -- сказал Торгов и, с испугом покраснев, стал говорить с панталонами. -- Свое время заедете-ка на панихиду! Я сам знаю, что там еще есть! Кому дали, коли меня льют? -- спросил Фома, -- я хочу сделать тебе\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая мысль, что они ничего не любят, но теперь это уже не котпич: его сын -- мудрец, который не знал, как он ни старался выступать. Санин посмотрел на нее и спросил, что \"большую\" речь. К тому же это, конечно, нужно для самого себя сделать что-то вроде содействии или\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело не касается содержания ее? Но я даже сам видел, как оно отрешено. Он мне не мешает говорить так, потому что я его обдумаю нарочно и возненавидеть тебя в точности. Я это говорю; это тоже недостойная мысль; но он же я идущий. .. он еще более пол\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно было не совсем, как будто только оно вглядывалось, все время. Покаживая к нему глазами, Матрена быстро проговорила: -- Это не надо, чтобы вы его любите. Он взял ее и тотчас же сел за столом; но, подойдя к столу, она молча, с краю дороги, закрыл лицо руками. На лице ее\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Eval @ step 1236] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия, как я вижу и знал; но не в духе он испытал, когда она сказала только: \"Нет, это от меня\", -- и в ней никогда не приходила женщина. \"А теперь я что-то говорил, -- продолжал он со вздохом, -- я видел тогда его так, как ее муж, а ты теперь\". .. Он подошел к окну и спросил ее:\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа, но от этого не было ни того, ни от кого. И вот, напротив, после сего я буду ходить к ней в кабинет, а то будет хорошо! Я ничего не знаю, о чем ты хочешь знать? Кого-то он убил отца, да и не должен! Ты любишь меня так, как тебе! -- Право, что я тебе\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания. Попросил кучера и предложил ему пирожное место. Все эти вопросы, в которые он находился, -- он это сказал, а он и сказал: \"А мы с тобой не говорили об этом\"? Что-то случилось. .. Но у меня было жарко и неловко, и все-таки я не мог понять ничего. .. Пр\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным. \"Может быть, это у него есть, это так же как-то странно, так: он хочет сказать, что именно в словах, вроде \"законы\", \"всем\", \"можно\" и \"черные дела\". Часа два тому назад он, не решавшись подслушивать \"калитин\" и \"Свет\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду петь. Да ты не сумасшедший: ты что мне скажешь? Нет, как жить в Петербурге! -- сказал он, хватая себя за руку. -- Я его убью, а ему не хочется идти к обеду на кулисы. -- Виноват, но все равно, что же! -- обратилась она к Соне с сво\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти. Ее слова, каких ты отлично определенностей не любишь, у нас, для меня только одна тварь; это мне не только жалко, но для тебя я чувствую, чтобы оно было для меня, если б не для тебя. Я от тебя в жизни знаю, ты меня ссорят с собою, и ты меня простужи.\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена. Я ничего не могу не сказать вам о твоем отношении с ним и еще раз и буду говорить с ним по-настоящему, — то есть, что я чувствую за собой, — но мне хотелось бы посчастливить его. Он не мог скрыть от него, а он уже давно мог узнать его и вдруг, как бы желая быть обиж\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая женщина с первою минуту сделала свои очищения. Имогена была уверена, что все мои родные вещи могли бы иметь право быть только в том месте, где бы можно было поскорее пройтись между собой, если б он был бы, что в нем не было этого происшествия; но теперь она не могла видеть, как прежде он видел\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело; если он не так-сказал, то и скажу ему: \"Совершенно в нем не было никакого, как я вас, родственника моего, который все-таки в деревне мне зависит, и что я был еще дельнее сего, чем прежде. .. Но ежели бы он не был тогда, так теперь и без него не имел пред\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно, она стала\n",
      "хозяйством, везде сирень. .. Зимой не пришло ей ни на кого. - Я тебе говорю, что такое в твоем сыне у нее? Неужели это не поедут? Не знаешь,\n",
      "говорят, что такое - так уж надо сказать, как у меня есть. - Разве я не вижу этого\n",
      "\n",
      "================================================================================\n",
      "\n",
      "{'eval_loss': 3.296207904815674, 'eval_runtime': 1.4883, 'eval_samples_per_second': 361.488, 'eval_steps_per_second': 45.69, 'epoch': 3.0}\n",
      "Perplexity: 27.010019906678487\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "if \"eval_loss\" in metrics:\n",
    "    print(\"Perplexity:\", math.exp(metrics[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b359a9",
   "metadata": {},
   "source": [
    "# Post-train SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6c55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "Завод 48. Материал: Фотография и изображение, а также видео. Магазин детского питания. Детские куриные блюда для детей от 5 лет. Состав. Для детей 1 год — пакетик; для детей 2-3 года — 5 пакетиков, 6-7 месяцев — 10 пакетиков, 9 лет — 15 пакетиков. 1. Организация: Услуги, услуги.\n",
      "\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "о сказке «небесная любовь» - Арифметика\n",
      "Читать: 4017.382\n",
      "Загадка «сказка небесная Любовь» - в этой вершине любви\n",
      "Мы с дочерью открыли в гостях у Роланда, который не только любил Сашу и Хабида, но и остался на твоем берегу, и мы его пригласили к нам, чтобы посидеть с\n",
      "\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "- Страница 35\n",
      "Возможно вы уже увидели подобных ряда, но я тоже не знала, как сделать крыжовник на 21 года. Недавно в интернете появился сайт, который помогает с бытовыми вопросами:\n",
      "http://www.rozbrukhnik.ru/roznik/ . Купить и сделать крыжовник. В нем много советов и примеров.\n",
      "Еще одна версия из интернета - http://www.silvestrov.ru/besplat\n",
      "\n",
      "Model Input 4:\n",
      "Как быстро выучить новый язык?\n",
      "Model Output 4:\n",
      "| Лексическое образование в Воронеже и Брянске\n",
      "Главная » Как быстро выучить новый язык?\n",
      "Как быстро выучить новый язык?\n",
      "Каждый из нас хочет быть уверенным, что умеет говорить на нужном языке. Некоторым умение говорить на другой язык становится неоставимой задачей для дальнейшего взаимопомощи с другими людьми. Но самое главное – сделать это быстро и с пользой для себя.\n",
      "Можно выбрать любую тему,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "def generate_answer(question):\n",
    "    prompt = question\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # уберём повтор промпта в начале (если модель его вернула)\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):].strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "for i, q in enumerate(questions_rus, start=1):\n",
    "    ans = generate_answer(q)\n",
    "    print(f\"Model Input {i}:\")\n",
    "    print(q)\n",
    "    print(f\"Model Output {i}:\")\n",
    "    print(ans)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ac20e",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Ты полезный ассистент. Отвечай на русском языке кратко и по делу.<|im_end|>\n",
      "<|im_start|>user\n",
      "Дайте три совета, как оставаться здоровым.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\n",
      "\n",
      "2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"d0rj/alpaca-cleaned-ru\"\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "def to_messages(example):\n",
    "    instruction = (example.get(\"instruction\") or \"\").strip()\n",
    "    inp = (example.get(\"input\") or \"\").strip()\n",
    "    output = (example.get(\"output\") or \"\").strip()\n",
    "\n",
    "    if inp:\n",
    "        user_text = instruction + \"\\n\\nВходные данные:\\n\" + inp\n",
    "    else:\n",
    "        user_text = instruction\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": output},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds_chat = ds.map(to_messages, remove_columns=ds.column_names)\n",
    "print(ds_chat[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8044da",
   "metadata": {},
   "source": [
    "# Дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfc7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51760/51760 [00:06<00:00, 7699.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Ты полезный ассистент. Отвечай на русском языке кратко и по делу.<|im_end|>\n",
      "<|im_start|>user\n",
      "Дайте три совета, как оставаться здоровым.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм не\n",
      "Train: 50724\n",
      "Val: 1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def apply_template(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds_text = ds_chat.map(apply_template, remove_columns=[\"messages\"])\n",
    "print(ds_text[0][\"text\"][:400])\n",
    "\n",
    "splits = ds_text.train_test_split(test_size=0.02, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Val:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d2b06d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 50724/50724 [00:02<00:00, 16992.15 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 50724/50724 [00:32<00:00, 1554.70 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 50724/50724 [00:00<00:00, 276266.69 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 1036/1036 [00:00<00:00, 11010.85 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1036/1036 [00:00<00:00, 1472.40 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1036/1036 [00:00<00:00, 167752.73 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='793' max='793' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [793/793 21:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>1.120233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=793, training_loss=1.17874399500266, metrics={'train_runtime': 1276.3682, 'train_samples_per_second': 39.741, 'train_steps_per_second': 0.621, 'total_flos': 5.384297075557171e+16, 'train_loss': 1.17874399500266})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "splits = ds_text.train_test_split(test_size=0.02, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"./qwen2_5_0_5b_sft_ru_safe\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    fp16=False,\n",
    "\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e95aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in Qwen2DecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "СsystemsystemsystemSystemsystemsystemsapisystemsystem systemsystemsystem::systemsystem_systemsystemsystemilibriumsystemsystemscsystemsystemsystemssystemsystemserversystemsystemrogsystemsystemtestsystemsystemspacesystemsystemstationsystemsystem administrationsystemsystemstitutionssystemsystemstsystemsystem-systemsystemsystemstatsystemsystem(systemsystemsimsystemsystemstitsystemsystemEnginesystemsystemstitutesystemsystem undersystemsystemvsystemsystemverssystemsystemservicesystemsystem securitysystemsystemtrsystemsystemmainsystemsystemSystemServicesystemsystemmodelsystemsystemicularsystemsystemrecsystemsystem-systemsystemsyssystemsystemsersystemsystemstatesystemsystem-ex\n",
      "\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "ВsystemsystemsystemSystemsystemsystemssystemsystem::systemsystem systemsystemsystemscsystemsystemilibriumsystemsystemstsystemsystem-exsystemsystemsystemssystemsystemservicesystemsystem_systemsystemsystemstatsystemsystemadsystemsystem securitysystemsystemmainsystemsystemSystemServicesystemsystemicularsystemsystem administrationsystemsystemspacesystemsystem mainsystemsystemtestsystemsystem(systemsystemserversystemsystemEnginesystemsystem_systemsystemsimsystemsystemrogsystemsystem\n",
      "systemsystemdefmodulesystemsystem->systemsystemstitsystemsystemstitutionssystemsystemdevicesystemsystem-systemsystemselfsystemsystem Administrationsystemsystemstatesystemsystemenginesystemsystemex\n",
      "\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "Кsystemsystemsystem systemsystemsystems::systemsystemscsystemsystemSystemsystemsystemspacesystemsystemstsystemsystemstatussystemsystemmainsystemsystemserversystemsystemadsystemsystemsystemssystemsystemsimsystemsystemstatsystemsystemselfsystemsystemrogsystemsystemservicesystemsystemilibriumsystemsystem::system administrationsystemsystemstitsystemsystemsyssystemsystemEnginesystemsystem_systemsystemsystem-exsystemsystem\tsystemsystemsystem->systemsystemvsystemsystem-tsystemsystemsedsystemsystemenginesystemsystemSystemServicesystemsystemexsystemsystemtsystemsystem-systemsystemsystemsubsystemsystemstatesystemsystem systemssystemsystem securitysystemsystem Systemsystem\n",
      "\n",
      "Model Input 4:\n",
      "Как быстро выучить новый язык?\n",
      "Model Output 4:\n",
      "Вsystemsystemsystem::systemsystem systemsystemsystemssystemsystemserversystemsystemSystemsystemsystemscsystemsystemsystemssystemsystem securitysystemsystemsubsystemsystemservicesystemsystem-exsystemsystemsimsystemsystem administrationsystemsystemstitutesystemsystemadsystemsystemmainsystemsystemstsystemsystemrogsystemsystemadminsystemsystemnosystemsystemilibriumsystemsystemverssystemsystem\n",
      "systemsystemstatsystemsystemSystemServicesystemsystemEnginesystemsystem_systemsystemsystemselfsystemsystem-systemsystemresystemsystemstitsystemsystem-tsystemsystem\tsystemsystemsystemdomainsystemsystem-systemsystemsystemenginesystemsystemspacesystemsystemexsystemsystemstatesystem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "def generate_answer(question, max_new_tokens=120):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(trainer.model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = trainer.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "\n",
    "            do_sample=True,          # <-- включаем sampling\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "\n",
    "            repetition_penalty=1.2,  # <-- анти-повторы\n",
    "            no_repeat_ngram_size=3,  # <-- запрещаем повтор 3-грамм\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    answer_ids = out_ids[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "for i, q in enumerate(questions_rus, 1):\n",
    "    ans = generate_answer(q)\n",
    "    print(f\"Model Input {i}:\")\n",
    "    print(q)\n",
    "    print(f\"Model Output {i}:\")\n",
    "    print(ans)\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
