{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3280bbde",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57378ce1",
   "metadata": {},
   "source": [
    "1) Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5213b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe28919",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/corpus\")\n",
    "OUT_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "\n",
    "MIN_SENT_CHARS = 20\n",
    "MAX_SENT_CHARS = 5000\n",
    "\n",
    "CONTEXT_LEN = 1024\n",
    "MAX_TOKENS_PER_CHUNK = CONTEXT_LEN - 2 \n",
    "\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"«\", '\"').replace(\"»\", '\"').replace(\"„\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"—\", \" — \")\n",
    "    text = text.replace(\"–\", \" — \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_punct(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
    "    s = re.sub(r\"!{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\?{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\?!){2,}\", \"?!\", s)\n",
    "    s = re.sub(r\",{2,}\", \",\", s)\n",
    "    s = re.sub(r\":{2,}\", \":\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([,.;:!?])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "CYR_RE = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def cyrillic_ratio(s):\n",
    "    letters = re.findall(r\"[A-Za-zА-Яа-яЁё]\", s)\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    cyr = sum(1 for ch in letters if CYR_RE.match(ch))\n",
    "    return cyr / len(letters)\n",
    "\n",
    "def is_good_sentence(s):\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if len(s) < MIN_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if len(s) > MAX_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if LATIN_RE.search(s):\n",
    "        return False\n",
    "    \n",
    "    if cyrillic_ratio(s) < 0.70:\n",
    "        return False\n",
    "    \n",
    "    letters_count = len(CYR_RE.findall(s))\n",
    "    if letters_count < 5:\n",
    "        return False\n",
    "    \n",
    "    if len(set(s)) <= 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_for_dedup(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9а-яё]+\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        return len(text.split())\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_sentences(sentences, max_tokens, tokenizer=None):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = count_tokens(s, tokenizer)\n",
    "        if s_tokens > max_tokens:\n",
    "            words = s.split()\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for w in words:\n",
    "                w_tokens = count_tokens(w, tokenizer)\n",
    "                if buf_tokens + w_tokens > max_tokens and buf:\n",
    "                    chunks.append(\" \".join(buf))\n",
    "                    buf = [w]\n",
    "                    buf_tokens = w_tokens\n",
    "                else:\n",
    "                    buf.append(w)\n",
    "                    buf_tokens += w_tokens\n",
    "\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + s_tokens > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            current_tokens = s_tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            current_tokens += s_tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecf0633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество файлов: 108\n",
      "=== STATS ===\n",
      "docs_total: 108\n",
      "docs_unique: 107\n",
      "sents_total: 537738\n",
      "sents_good: 441048\n",
      "sents_unique: 436042\n",
      "chunks_total: 6386\n",
      "Saved: data/pretrain_corpus.jsonl chunks: 6386\n"
     ]
    }
   ],
   "source": [
    "txt_files = sorted(DATA_PATH.glob(\"*.txt\"))\n",
    "print(\"Количество файлов:\", len(txt_files))\n",
    "\n",
    "seen_docs = set()\n",
    "seen_sents = set()\n",
    "all_chunks = []\n",
    "\n",
    "stats = {\n",
    "    \"docs_total\": 0,\n",
    "    \"docs_unique\": 0,\n",
    "    \"sents_total\": 0,\n",
    "    \"sents_good\": 0,\n",
    "    \"sents_unique\": 0,\n",
    "    \"chunks_total\": 0\n",
    "}\n",
    "\n",
    "for fp in txt_files:\n",
    "    stats[\"docs_total\"] += 1\n",
    "\n",
    "    raw = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = normalize_text(raw)\n",
    "\n",
    "    doc_key = sha1(normalize_for_dedup(raw))\n",
    "    if doc_key in seen_docs:\n",
    "        continue\n",
    "\n",
    "    seen_docs.add(doc_key)\n",
    "    stats[\"docs_unique\"] += 1\n",
    "\n",
    "    sents = split_sentences(raw)\n",
    "    stats[\"sents_total\"] += len(sents)\n",
    "\n",
    "    cleaned = []\n",
    "    for s in sents:\n",
    "        s = normalize_punct(s)\n",
    "        if not is_good_sentence(s):\n",
    "            continue\n",
    "\n",
    "        stats[\"sents_good\"] += 1\n",
    "\n",
    "        sent_key = sha1(normalize_for_dedup(s))\n",
    "        if sent_key in seen_sents:\n",
    "            continue\n",
    "\n",
    "        seen_sents.add(sent_key)\n",
    "        stats[\"sents_unique\"] += 1\n",
    "        cleaned.append(s)\n",
    "\n",
    "    chunks = chunk_sentences(cleaned, max_tokens=MAX_TOKENS_PER_CHUNK, tokenizer=None)\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = f\"{BOS} {ch.strip()} {EOS}\"\n",
    "        all_chunks.append(text)\n",
    "\n",
    "stats[\"chunks_total\"] = len(all_chunks)\n",
    "\n",
    "print(\"=== STATS ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for t in all_chunks:\n",
    "        f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcae1a2",
   "metadata": {},
   "source": [
    "3) Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843125e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово. Строк для обучения токенизатора: 6386\n",
      "Файл: data/tokenizer_train.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "JSONL_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "TXT_TRAIN_PATH = Path(\"./data/tokenizer_train.txt\")\n",
    "\n",
    "TXT_TRAIN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f_in, open(TXT_TRAIN_PATH, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        f_out.write(text.replace(\"\\n\", \" \") + \"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Готово. Строк для обучения токенизатора:\", count)\n",
    "print(\"Файл:\", TXT_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9eff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Saved tokenizer.json: tokenizer_bpe_3k/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "VOCAB_SIZE = 3000\n",
    "MIN_FREQUENCY = 2\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[str(TXT_TRAIN_PATH)],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "\n",
    "OUT_DIR = Path(\"./tokenizer_bpe_3k\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer.save_model(str(OUT_DIR))\n",
    "\n",
    "tokenizer.save(str(OUT_DIR / \"tokenizer.json\"))\n",
    "print(\"Saved tokenizer.json:\", OUT_DIR / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e28689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 6386 examples [00:00, 40259.95 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:05<00:00, 1086.98 examples/s]\n",
      "Map: 100%|██████████| 6386/6386 [00:11<00:00, 569.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 26876\n",
      "})\n",
      "Примеров: 26876\n",
      "Длина блока: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\": \"./data/pretrain_corpus.jsonl\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "tokenized = ds[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_input_ids = []\n",
    "    concatenated_attention = []\n",
    "\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated_input_ids.extend(ids)\n",
    "\n",
    "    for am in examples[\"attention_mask\"]:\n",
    "        concatenated_attention.extend(am)\n",
    "\n",
    "    total_length = len(concatenated_input_ids)\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, total_length, BLOCK_SIZE):\n",
    "        chunk_ids = concatenated_input_ids[i : i + BLOCK_SIZE]\n",
    "        chunk_mask = concatenated_attention[i : i + BLOCK_SIZE]\n",
    "\n",
    "        input_ids.append(chunk_ids)\n",
    "        attention_mask.append(chunk_mask)\n",
    "        labels.append(chunk_ids.copy())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "lm_dataset = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized.column_names\n",
    ")\n",
    "\n",
    "print(lm_dataset)\n",
    "print(\"Примеров:\", len(lm_dataset))\n",
    "print(\"Длина блока:\", len(lm_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501278",
   "metadata": {},
   "source": [
    "4. Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d04509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3000\n",
      "Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "Total params: 132006912\n",
      "Trainable params: 132006912\n",
      "Params: 132.01\n",
      "Logits shape: torch.Size([2, 32, 3000])\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer_bpe_3k/tokenizer.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    rms_norm_eps=1e-6,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total params:\", total_params)\n",
    "print(\"Trainable params:\", trainable_params)\n",
    "print(\"Params:\", round(total_params / 1e6, 2))\n",
    "\n",
    "model.eval()\n",
    "x = torch.randint(0, tokenizer.vocab_size, (2, 32))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=x)\n",
    "print(\"Logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce82bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8c1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 26338\n",
      "Val size: 538\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]\n",
    "\n",
    "splits = lm_dataset.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c057ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PromptGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, prompts, tokenizer, max_new_tokens=80):\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        device = model.device\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[Eval @ step {state.global_step}] Prompt generations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        old_cache = getattr(model.config, \"use_cache\", False)\n",
    "        model.config.use_cache = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                out_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"\\n--- Prompt #{i+1} ---\")\n",
    "                print(\"PROMPT:\", prompt)\n",
    "                print(\"GEN:\\n\", text)\n",
    "\n",
    "        model.config.use_cache = old_cache\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3ec39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19241/879651457.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model.config.use_cache = False\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "callbacks = [PromptGenerationCallback(test_prompts, tokenizer, max_new_tokens=80)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b47d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1236' max='1236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1236/1236 10:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.579600</td>\n",
       "      <td>3.608132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.118000</td>\n",
       "      <td>3.316448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Eval @ step 500] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия на лечение ее существ, - для меня все остальное, чтобы уходить директором. Их всегда, что эта история наружность и что не достигаемая фраза жизни\". Он читал, но когда он остроумно обижался, она еще села в супруг, и, наконец, в сущности\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа в Вене, на площадку с черным крохотным ножом. И все это были так близко: он, как всегда, спрятал у себя шпарки. Кучер, разнообразные лекции (\"только не могу забывать\", - со вздохами подумал Логин с подчеркнутой роскош\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания в школе, что это такое же слово, -- то есть к отцу, в первых дней и в сущности соображала его; но в сущности он только хотел знать ее в жизни, как будто желая уезжать в своем деле, даже на все недоверчиво, неуместная, но совершенно безопасная для него. Врон\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным, с трудом ожидает его: -- Узнать, что есть такие мармированы. Танкред видел, как только он, когда в комнате, ушел, у него как будто бы тягучее и ясное лицо пела к дому на груди, глаза ее, все еще ближе становились на клетчатые мукушки, и\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду перешагнуться с места в отделении. Я не понимаю ее, он стал уверен, что он уже не видит. .. Он умел думать, что его жена меня любит. Он очень любил его. Она сказала это: — Вы все-таки обманываете эту новость! Пожалуй, я тебя не слыхал. Но она сама решила\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти. На немецкое, наивное, почти злобно, точно, как и они, и не могут жить. Это так просто и весело обожало его: это было даже очень жажда. И потом как будто не было ничего, что он так любит меня на себе, что не только к тому, что все к тому, когда у него там уединилась. Это\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена, и потому она по ее мнениям не мог. Этот человек очень понравится, когда она будет со мной и как в этом году. Я только теперь имел то же, чего это было бы и на меня смотреть в Петербург, а еще более как будто он говорит: \"Хоть вы сами уже заходите\". Ну так и то#есть да что вы,\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая идеальная комната. В эту минуту он чувствовал это в глазах ее речи, когда его был очень больный вопрос: \"бежать! .. .\" И вдруг его не хотелось бы говорить: \"А у меня она не пишет, что я с ним в виде его! \" -- \"Мне так много\", но она меня не хотела бы было, чтобы она\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело. Воспользование председательнейшими людьми, я всегда в жизни, когда у меня есть в мире отличие и осуждения. Строим мы с ними не поет ни. Я так не видел, что в этой средине. Мне было сказано: я был сокровен. При чем больше я, как только\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно, я должен был быть женой; но когда я в нем думал с вами, когда я предлагаю правду, чтобы так и выйти из Москвы к моим братам. Но все-таки и я с ужасом уверен, что у меня есть до того, как пройдет через голову, на базаре. И что вы не поворачиваете\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[Eval @ step 1000] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия. До сего дня мы оба вышли в кабинет. За это время уже можно было видеть Лизавету Прокофьевну. — А я вас не знаю, ты мне не нравился, чтобы мне не хочется меня видеть, -- прошептала она, взглянув на Марью Николаевну, которая хотела поднять глаза и с выражением недовольного смущения\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа и, как всегда, невыносимо, только по одному чувству. Да и то, что, при котором не было ни одного слова, он не мог понимать этого. .. Но Левин видел, что это была та самая особенная мысль, которую он знал. Кити уже были в новом положении. Но теперь это она поняла. Она думала о любви к\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания в роли у ворот. \"Куда я тут? \" спросил он у подъезжавшего канавки. Узнав его, он сказал: - Вчера вечером, если б вы знали в нем новое время, я бы вам скажу, чтобы я тебе сейчас же узнал. Он взглянул на меня в сизый час со всеми кас\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным, что я ничего не могу жить в другом, не знаю. И наше время: у кого же мне супруга? Зачем же ты теперь знаешь, что для меня, по крайней мере, неужели? Ведь я все-таки хочу тебя прожить. Я уже не буду больше и не буду, когда я влюбилась, хотя и думала, что\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду ходить. В это время кто-то засмеялся. Приподнялась пешком и тихо сказала: -- Ей что же дальше, -- вот какие! И снова собралась женщина, бросила письмо на постель, потом по лестнице, к трюмо, и быстро пошел прочь от дыхания. Он стоял передо мною, и, наконец,\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти и не давать ему возможным. .. -- сказал отец Андрей, подтвердительно глядя на него. Он встал и с досадой поглядел на старика в черной квадрате, стоявшей у ворот; и, по-моему, старухе он еще не успел заметить, как казалось, что все они делают ему, но тот, весь\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена: что будет в бока? .. -- сказал Торгов и, с испугом покраснев, стал говорить с панталонами. -- Свое время заедете-ка на панихиду! Я сам знаю, что там еще есть! Кому дали, коли меня льют? -- спросил Фома, -- я хочу сделать тебе\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая мысль, что они ничего не любят, но теперь это уже не котпич: его сын -- мудрец, который не знал, как он ни старался выступать. Санин посмотрел на нее и спросил, что \"большую\" речь. К тому же это, конечно, нужно для самого себя сделать что-то вроде содействии или\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело не касается содержания ее? Но я даже сам видел, как оно отрешено. Он мне не мешает говорить так, потому что я его обдумаю нарочно и возненавидеть тебя в точности. Я это говорю; это тоже недостойная мысль; но он же я идущий. .. он еще более пол\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно было не совсем, как будто только оно вглядывалось, все время. Покаживая к нему глазами, Матрена быстро проговорила: -- Это не надо, чтобы вы его любите. Он взял ее и тотчас же сел за столом; но, подойдя к столу, она молча, с краю дороги, закрыл лицо руками. На лице ее\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Eval @ step 1236] Prompt generations\n",
      "================================================================================\n",
      "\n",
      "--- Prompt #1 ---\n",
      "PROMPT: Все мысли, которые имеют огромные последствия\n",
      "GEN:\n",
      " Все мысли, которые имеют огромные последствия, как я вижу и знал; но не в духе он испытал, когда она сказала только: \"Нет, это от меня\", -- и в ней никогда не приходила женщина. \"А теперь я что-то говорил, -- продолжал он со вздохом, -- я видел тогда его так, как ее муж, а ты теперь\". .. Он подошел к окну и спросил ее:\n",
      "\n",
      "--- Prompt #2 ---\n",
      "PROMPT: Сила войска зависит от его духа\n",
      "GEN:\n",
      " Сила войска зависит от его духа, но от этого не было ни того, ни от кого. И вот, напротив, после сего я буду ходить к ней в кабинет, а то будет хорошо! Я ничего не знаю, о чем ты хочешь знать? Кого-то он убил отца, да и не должен! Ты любишь меня так, как тебе! -- Право, что я тебе\n",
      "\n",
      "--- Prompt #3 ---\n",
      "PROMPT: Мысль о том, что он принес страдания\n",
      "GEN:\n",
      " Мысль о том, что он принес страдания. Попросил кучера и предложил ему пирожное место. Все эти вопросы, в которые он находился, -- он это сказал, а он и сказал: \"А мы с тобой не говорили об этом\"? Что-то случилось. .. Но у меня было жарко и неловко, и все-таки я не мог понять ничего. .. Пр\n",
      "\n",
      "--- Prompt #4 ---\n",
      "PROMPT: Человек сознает себя свободным\n",
      "GEN:\n",
      " Человек сознает себя свободным. \"Может быть, это у него есть, это так же как-то странно, так: он хочет сказать, что именно в словах, вроде \"законы\", \"всем\", \"можно\" и \"черные дела\". Часа два тому назад он, не решавшись подслушивать \"калитин\" и \"Свет\n",
      "\n",
      "--- Prompt #5 ---\n",
      "PROMPT: Что бы ни случилось, я всегда буду\n",
      "GEN:\n",
      " Что бы ни случилось, я всегда буду петь. Да ты не сумасшедший: ты что мне скажешь? Нет, как жить в Петербурге! -- сказал он, хватая себя за руку. -- Я его убью, а ему не хочется идти к обеду на кулисы. -- Виноват, но все равно, что же! -- обратилась она к Соне с сво\n",
      "\n",
      "--- Prompt #6 ---\n",
      "PROMPT: Любовь мешает смерти\n",
      "GEN:\n",
      " Любовь мешает смерти. Ее слова, каких ты отлично определенностей не любишь, у нас, для меня только одна тварь; это мне не только жалко, но для тебя я чувствую, чтобы оно было для меня, если б не для тебя. Я от тебя в жизни знаю, ты меня ссорят с собою, и ты меня простужи.\n",
      "\n",
      "--- Prompt #7 ---\n",
      "PROMPT: Нет, жизнь не кончена\n",
      "GEN:\n",
      " Нет, жизнь не кончена. Я ничего не могу не сказать вам о твоем отношении с ним и еще раз и буду говорить с ним по-настоящему, — то есть, что я чувствую за собой, — но мне хотелось бы посчастливить его. Он не мог скрыть от него, а он уже давно мог узнать его и вдруг, как бы желая быть обиж\n",
      "\n",
      "--- Prompt #8 ---\n",
      "PROMPT: Всякая мысль, даже самая простая\n",
      "GEN:\n",
      " Всякая мысль, даже самая простая женщина с первою минуту сделала свои очищения. Имогена была уверена, что все мои родные вещи могли бы иметь право быть только в том месте, где бы можно было поскорее пройтись между собой, если б он был бы, что в нем не было этого происшествия; но теперь она не могла видеть, как прежде он видел\n",
      "\n",
      "--- Prompt #9 ---\n",
      "PROMPT: Война не любезность, а самое гадкое дело\n",
      "GEN:\n",
      " Война не любезность, а самое гадкое дело; если он не так-сказал, то и скажу ему: \"Совершенно в нем не было никакого, как я вас, родственника моего, который все-таки в деревне мне зависит, и что я был еще дельнее сего, чем прежде. .. Но ежели бы он не был тогда, так теперь и без него не имел пред\n",
      "\n",
      "--- Prompt #10 ---\n",
      "PROMPT: Чтобы жить честно\n",
      "GEN:\n",
      " Чтобы жить честно, она стала\n",
      "хозяйством, везде сирень. .. Зимой не пришло ей ни на кого. - Я тебе говорю, что такое в твоем сыне у нее? Неужели это не поедут? Не знаешь,\n",
      "говорят, что такое - так уж надо сказать, как у меня есть. - Разве я не вижу этого\n",
      "\n",
      "================================================================================\n",
      "\n",
      "{'eval_loss': 3.296207904815674, 'eval_runtime': 1.4883, 'eval_samples_per_second': 361.488, 'eval_steps_per_second': 45.69, 'epoch': 3.0}\n",
      "Perplexity: 27.010019906678487\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "if \"eval_loss\" in metrics:\n",
    "    print(\"Perplexity:\", math.exp(metrics[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b359a9",
   "metadata": {},
   "source": [
    "# Post-train SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9953da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "8\n",
      "Согласна, что звучит достаточно удачно, так как у нас есть такая возможность, когда мы разберемся со всеми заданиями, которые нам надо выполнить в этой теме. А потом будет тяжелейшая задача - это то, что в этом разделе участвуют теоретики. В общем, в этой категории мы много говорим о линии длины.\n",
      "\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "Сколько бабок у тебя есть?\n",
      "У меня два\n",
      "Один ты не видел,\n",
      "Другый я тебе\n",
      "Помню, как ты говорил.\n",
      "Или сказала...\n",
      "Ответ:\n",
      "Сколько мы вылезали в эту землю,\n",
      "Что нас обожаете,\n",
      "Где мы теснелись,\n",
      "Лучше всего были ужасные неприятности?\n",
      "Когда это прошло,\n",
      "Каким бы я ни был, то открылся один душой,\n",
      "Будь он моей или нет\n",
      "\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "29 октября 2013 г.\n",
      "\n",
      "Крыжовники нужны для смены баллонов. Кто их собрал, тот сам их подает.\n",
      "\n",
      "Model Input 4:\n",
      "Как быстро выучить новый язык?\n",
      "Model Output 4:\n",
      "Скорость зависит от многих факторов, среди которых, прежде всего, наличие необходимого количества времени. Кроме того, необходимо понять, что в данном случае наиболее эффективными является метод коммуникации, который использует только улыбка и разговорное время, когда слушатель может воспринимать информацию и ее собирать с ней в уме. Важно также учесть то, что для начала необходимо правильно подготовиться к обучению нового языка, ведь сначала нужно освоить основные пункты речи\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "def generate_answer(question, max_new_tokens=120):\n",
    "    prompt = f\"{SYSTEM_PROMPT}\\nВопрос: {question}\\nОтвет:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.15,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    answer_ids = out_ids[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "for i, q in enumerate(questions_rus, start=1):\n",
    "    print(f\"Model Input {i}:\")\n",
    "    print(q)\n",
    "    print(f\"Model Output {i}:\")\n",
    "    print(generate_answer(q))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ac20e",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8c28f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51760/51760 [00:03<00:00, 16678.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Ты полезный ассистент. Отвечай на русском языке кратко и по делу.', 'role': 'system'}, {'content': 'Дайте три совета, как оставаться здоровым.', 'role': 'user'}, {'content': '1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.', 'role': 'assistant'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "def to_chat(example):\n",
    "    instruction = (example.get(\"instruction\") or \"\").strip()\n",
    "    inp = (example.get(\"input\") or \"\").strip()\n",
    "    output = (example.get(\"output\") or \"\").strip()\n",
    "\n",
    "    user_text = instruction\n",
    "    if inp:\n",
    "        user_text = f\"{instruction}\\n\\n{inp}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": output},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "chat_ds = ds.map(to_chat, remove_columns=ds.column_names)\n",
    "\n",
    "print(chat_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "637fd899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 12940/12940 [00:01<00:00, 7687.08 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 12940/12940 [00:10<00:00, 1265.35 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 12940/12940 [00:00<00:00, 102595.39 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1618' max='1618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1618/1618 26:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.346900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.249900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>1.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.231200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('qwen2.5-0.5b-sft-ru/tokenizer_config.json',\n",
       " 'qwen2.5-0.5b-sft-ru/special_tokens_map.json',\n",
       " 'qwen2.5-0.5b-sft-ru/chat_template.jinja',\n",
       " 'qwen2.5-0.5b-sft-ru/vocab.json',\n",
       " 'qwen2.5-0.5b-sft-ru/merges.txt',\n",
       " 'qwen2.5-0.5b-sft-ru/added_tokens.json',\n",
       " 'qwen2.5-0.5b-sft-ru/tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "\n",
    "def to_chat(example):\n",
    "    instruction = (example.get(\"instruction\") or \"\").strip()\n",
    "    inp = (example.get(\"input\") or \"\").strip()\n",
    "    output = (example.get(\"output\") or \"\").strip()\n",
    "\n",
    "    user_text = instruction\n",
    "    if inp:\n",
    "        user_text = f\"{instruction}\\n\\n{inp}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": output},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "train_ds = ds.map(to_chat, remove_columns=ds.column_names)\n",
    "train_ds = train_ds.select(range(len(train_ds) // 4))\n",
    "\n",
    "def formatting_func(example):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "cfg_kwargs = dict(\n",
    "    output_dir=\"qwen2.5-0.5b-sft-ru\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "cfg_params = inspect.signature(SFTConfig.__init__).parameters\n",
    "if \"max_seq_length\" in cfg_params:\n",
    "    cfg_kwargs[\"max_seq_length\"] = 512\n",
    "elif \"max_length\" in cfg_params:\n",
    "    cfg_kwargs[\"max_length\"] = 512\n",
    "\n",
    "if \"bf16\" in cfg_params:\n",
    "    cfg_kwargs[\"bf16\"] = torch.cuda.is_available()\n",
    "if \"fp16\" in cfg_params:\n",
    "    cfg_kwargs[\"fp16\"] = False\n",
    "if \"optim\" in cfg_params:\n",
    "    cfg_kwargs[\"optim\"] = \"adamw_torch\"\n",
    "\n",
    "sft_config = SFTConfig(**cfg_kwargs)\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_ds,\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "trainer_params = inspect.signature(SFTTrainer.__init__).parameters\n",
    "if \"processing_class\" in trainer_params:\n",
    "    trainer_kwargs[\"processing_class\"] = tokenizer\n",
    "elif \"tokenizer\" in trainer_params:\n",
    "    trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "\n",
    "trainer = SFTTrainer(**trainer_kwargs)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"qwen2.5-0.5b-sft-ru\")\n",
    "tokenizer.save_pretrained(\"qwen2.5-0.5b-sft-ru\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8cdcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'qwen2.5-0.5b-sft-ru' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "Солнечная система состоит из 87 сферических планет, известных как «Галактические планеты». Они разделены на два семейства: «Земные» (в основном Земля) и «Азотных», которые находятся в диаметральном расположении. Всего есть 86 сферические планета в созвездии Сатурн-Кубань.\n",
      "\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "Дорогой читатель, уважаемый земляк,\n",
      "\n",
      "Это мое воспоминание о том, как впервые встретилась с моим другом, который был таким потрясающим, что я никогда не забуду его. Я был еще ребенком, когда мы были друзьями, и мы вместе исследовали страну, отмечая каждый день его приключениями.\n",
      "\n",
      "Мне нравилось общение с вашей страной, особенно изучив ее богатую историю и культуру. И\n",
      "\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "Собирая крыжовообразование, вам нужно взять несколько мелочей из различных предметов в вашем доме или пространстве. Вот некоторые вещи, которые вы можете найти:\n",
      "\n",
      "1. Крошки для пилок: эти крошечные крахмалы с белыми пятнышками, хорошо нанесенные на поверхность, служат отличной начинкой для крыжового оружия.\n",
      "\n",
      "2. Беренгамо (червь): черви являются важным компонентом крыжев\n",
      "\n",
      "Model Input 4:\n",
      "Как быстро выучить новый язык?\n",
      "Model Output 4:\n",
      "Современные методы обучения можно разделить на несколько этапов, которые могут помочь вам быстро изучать новый язык:\n",
      "\n",
      "1. Понимание основных концепций: первое, что нужно сделать, это понять основные концепции нового языка. Это может быть включая его диалект, поговорки, фразы и другие элементы. Если у вас есть основная информация о новом языках, это может быть особенно удобным.\n",
      "\n",
      "2. Практический подход. Используйте практи\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"qwen2.5-0.5b-sft-ru\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты полезный ассистент. Отвечай на русском языке кратко и по делу.\"\n",
    "\n",
    "def generate_answer(question, max_new_tokens=120):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.15,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    answer_ids = out_ids[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "for i, q in enumerate(questions_rus, start=1):\n",
    "    print(f\"Model Input {i}:\")\n",
    "    print(q)\n",
    "    print(f\"Model Output {i}:\")\n",
    "    print(generate_answer(q))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
