{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3280bbde",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57378ce1",
   "metadata": {},
   "source": [
    "1) Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe28919",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/test\")\n",
    "OUT_PATH = Path(\"./data/pretrain_corpus.jsonl\")\n",
    "\n",
    "MIN_SENT_CHARS = 20\n",
    "MAX_SENT_CHARS = 5000\n",
    "\n",
    "CONTEXT_LEN = 1024\n",
    "MAX_TOKENS_PER_CHUNK = CONTEXT_LEN - 2 \n",
    "\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"«\", '\"').replace(\"»\", '\"').replace(\"„\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"—\", \" — \")\n",
    "    text = text.replace(\"–\", \" — \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_punct(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
    "    s = re.sub(r\"!{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\?{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\?!){2,}\", \"?!\", s)\n",
    "    s = re.sub(r\",{2,}\", \",\", s)\n",
    "    s = re.sub(r\":{2,}\", \":\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([,.;:!?])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "CYR_RE = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def cyrillic_ratio(s):\n",
    "    letters = re.findall(r\"[A-Za-zА-Яа-яЁё]\", s)\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    cyr = sum(1 for ch in letters if CYR_RE.match(ch))\n",
    "    return cyr / len(letters)\n",
    "\n",
    "def is_good_sentence(s):\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if len(s) < MIN_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if len(s) > MAX_SENT_CHARS:\n",
    "        return False\n",
    "\n",
    "    if LATIN_RE.search(s):\n",
    "        return False\n",
    "    \n",
    "    if cyrillic_ratio(s) < 0.70:\n",
    "        return False\n",
    "    \n",
    "    letters_count = len(CYR_RE.findall(s))\n",
    "    if letters_count < 5:\n",
    "        return False\n",
    "    \n",
    "    if len(set(s)) <= 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_for_dedup(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9а-яё]+\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        return len(text.split())\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_sentences(sentences, max_tokens, tokenizer=None):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = count_tokens(s, tokenizer)\n",
    "        if s_tokens > max_tokens:\n",
    "            words = s.split()\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for w in words:\n",
    "                w_tokens = count_tokens(w, tokenizer)\n",
    "                if buf_tokens + w_tokens > max_tokens and buf:\n",
    "                    chunks.append(\" \".join(buf))\n",
    "                    buf = [w]\n",
    "                    buf_tokens = w_tokens\n",
    "                else:\n",
    "                    buf.append(w)\n",
    "                    buf_tokens += w_tokens\n",
    "\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + s_tokens > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            current_tokens = s_tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            current_tokens += s_tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf0633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество файлов: 1\n",
      "=== STATS ===\n",
      "docs_total: 1\n",
      "docs_unique: 1\n",
      "sents_total: 667\n",
      "sents_good: 508\n",
      "sents_unique: 508\n",
      "chunks_total: 5\n",
      "Saved: data\\pretrain_corpus.jsonl chunks: 5\n"
     ]
    }
   ],
   "source": [
    "txt_files = sorted(DATA_PATH.glob(\"*.txt\"))\n",
    "print(\"Количество файлов:\", len(txt_files))\n",
    "\n",
    "seen_docs = set()\n",
    "seen_sents = set()\n",
    "all_chunks = []\n",
    "\n",
    "stats = {\n",
    "    \"docs_total\": 0,\n",
    "    \"docs_unique\": 0,\n",
    "    \"sents_total\": 0,\n",
    "    \"sents_good\": 0,\n",
    "    \"sents_unique\": 0,\n",
    "    \"chunks_total\": 0\n",
    "}\n",
    "\n",
    "for fp in txt_files:\n",
    "    stats[\"docs_total\"] += 1\n",
    "\n",
    "    raw = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = normalize_text(raw)\n",
    "\n",
    "    doc_key = sha1(normalize_for_dedup(raw))\n",
    "    if doc_key in seen_docs:\n",
    "        continue\n",
    "\n",
    "    seen_docs.add(doc_key)\n",
    "    stats[\"docs_unique\"] += 1\n",
    "\n",
    "    sents = split_sentences(raw)\n",
    "    stats[\"sents_total\"] += len(sents)\n",
    "\n",
    "    cleaned = []\n",
    "    for s in sents:\n",
    "        s = normalize_punct(s)\n",
    "        if not is_good_sentence(s):\n",
    "            continue\n",
    "\n",
    "        stats[\"sents_good\"] += 1\n",
    "\n",
    "        sent_key = sha1(normalize_for_dedup(s))\n",
    "        if sent_key in seen_sents:\n",
    "            continue\n",
    "\n",
    "        seen_sents.add(sent_key)\n",
    "        stats[\"sents_unique\"] += 1\n",
    "        cleaned.append(s)\n",
    "\n",
    "    chunks = chunk_sentences(cleaned, max_tokens=MAX_TOKENS_PER_CHUNK, tokenizer=None)\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = f\"{BOS} {ch.strip()} {EOS}\"\n",
    "        all_chunks.append(text)\n",
    "\n",
    "stats[\"chunks_total\"] = len(all_chunks)\n",
    "\n",
    "print(\"=== STATS ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for t in all_chunks:\n",
    "        f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks: 5\n",
      "min: 769\n",
      "max: 1024\n",
      "avg: 970.6\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
